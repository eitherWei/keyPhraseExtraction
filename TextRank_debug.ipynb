{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation of TextRank\n",
    "(Based on: https://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input text is given below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source of text:\n",
    "#https://www.researchgate.net/publication/227988510_Automatic_Keyword_Extraction_from_Individual_Documents\n",
    "\n",
    "Text = \"a b c d b c\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Text Data\n",
    "\n",
    "The raw input text is cleaned off non-printable characters (if any) and turned into lower case.\n",
    "The processed input text is then tokenized using NLTK library functions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text: \n",
      "\n",
      "['a', 'b', 'c', 'd', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "import string\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "def clean(text):\n",
    "    text = text.lower()\n",
    "    printable = set(string.printable)\n",
    "    text = list(filter(lambda x: x in printable, text)) #filter funny characters, if any.\n",
    "    return \"\".join(text)\n",
    "\n",
    "Cleaned_text = clean(Text)\n",
    "\n",
    "text = word_tokenize(Cleaned_text)\n",
    "\n",
    "print(\"Tokenized Text: \\n\")\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging For Lemmatization\n",
    "\n",
    "NLTK is again used for <b>POS tagging</b> the input text so that the words can be lemmatized based on their POS tags.\n",
    "\n",
    "Description of POS tags: \n",
    "\n",
    "\n",
    "http://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Text with POS tags: \n",
      "\n",
      "[('a', 'DT'), ('b', 'NN'), ('c', 'NN'), ('d', 'NN'), ('b', 'NN'), ('c', 'NN')]\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('averaged_perceptron_tagger')\n",
    "  \n",
    "POS_tag = nltk.pos_tag(text)\n",
    "\n",
    "print(\"Tokenized Text with POS tags: \\n\")\n",
    "print(POS_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "The tokenized text (mainly the nouns and adjectives) is normalized by <b>lemmatization</b>.\n",
    "In lemmatization different grammatical counterparts of a word will be replaced by single\n",
    "basic lemma. For example, 'glasses' may be replaced by 'glass'. \n",
    "\n",
    "Details about lemmatization: \n",
    "    \n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text tokens after lemmatization of adjectives and nouns: \n",
      "\n",
      "['a', 'b', 'c', 'd', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "#nltk.download('wordnet')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "adjective_tags = ['JJ','JJR','JJS']\n",
    "\n",
    "lemmatized_text = []\n",
    "\n",
    "for word in POS_tag:\n",
    "    if word[1] in adjective_tags:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0],pos=\"a\")))\n",
    "    else:\n",
    "        lemmatized_text.append(str(wordnet_lemmatizer.lemmatize(word[0]))) #default POS = noun\n",
    "        \n",
    "print(\"Text tokens after lemmatization of adjectives and nouns: \\n\")\n",
    "print(lemmatized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary Creation\n",
    "\n",
    "Vocabulary will only contain unique words from processed_text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'a', 'd']\n"
     ]
    }
   ],
   "source": [
    "vocabulary = list(set(lemmatized_text))\n",
    "print(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Graph\n",
    "\n",
    "TextRank is a graph based model, and thus it requires us to build a graph. Each words in the vocabulary will serve as a vertex for graph. The words will be represented in the vertices by their index in vocabulary list.  \n",
    "\n",
    "The weighted_edge matrix contains the information of edge connections among all vertices.\n",
    "I am building wieghted undirected edges.\n",
    "\n",
    "weighted_edge[i][j] contains the weight of the connecting edge between the word vertex represented by vocabulary index i and the word vertex represented by vocabulary j.\n",
    "\n",
    "If weighted_edge[i][j] is zero, it means no edge connection is present between the words represented by index i and j.\n",
    "\n",
    "There is a connection between the words (and thus between i and j which represents them) if the words co-occur within a window of a specified 'window_size' in the processed_text.\n",
    "\n",
    "The value of the weighted_edge[i][j] is increased by (1/(distance between positions of words currently represented by i and j)) for every connection discovered between the same words in different locations of the text. \n",
    "\n",
    "The covered_coocurrences list (which is contain the list of pairs of absolute positions in processed_text of the words whose coocurrence at that location is already checked) is managed so that the same two words located in the same positions in processed_text are not repetitively counted while sliding the window one text unit at a time.\n",
    "\n",
    "The score of all vertices are intialized to one. \n",
    "\n",
    "Self-connections are not considered, so weighted_edge[i][i] will be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i b\n",
      "j b\n",
      "next loop\n",
      "\n",
      "i b\n",
      "j c\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "counted\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "counted\n",
      "next loop\n",
      "\n",
      "i b\n",
      "j a\n",
      "['a', 'b']\n",
      "counted\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i b\n",
      "j d\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "counted\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i c\n",
      "j b\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "counted\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "counted\n",
      "next loop\n",
      "\n",
      "i c\n",
      "j c\n",
      "next loop\n",
      "\n",
      "i c\n",
      "j a\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i c\n",
      "j d\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "counted\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i a\n",
      "j b\n",
      "['a', 'b']\n",
      "counted\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i a\n",
      "j c\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i a\n",
      "j a\n",
      "next loop\n",
      "\n",
      "i a\n",
      "j d\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i d\n",
      "j b\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "counted\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i d\n",
      "j c\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "counted\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i d\n",
      "j a\n",
      "['a', 'b']\n",
      "['b', 'c']\n",
      "['c', 'd']\n",
      "['d', 'b']\n",
      "['b', 'c']\n",
      "next loop\n",
      "\n",
      "i d\n",
      "j d\n",
      "next loop\n",
      "\n",
      "['b', 'c', 'a', 'd']\n",
      "\n",
      "weighted matrix\n",
      " [[0. 2. 1. 1.]\n",
      " [2. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "vocab_len = len(vocabulary)\n",
    "\n",
    "weighted_edge = np.zeros((vocab_len,vocab_len),dtype=np.float32)\n",
    "\n",
    "score = np.zeros((vocab_len),dtype=np.float32)\n",
    "window_size = 2\n",
    "covered_coocurrences = []\n",
    "\n",
    "for i in range(0,vocab_len):\n",
    "    score[i]=1\n",
    "    for j in range(0,vocab_len):\n",
    "        print(\"i\",vocabulary[i])\n",
    "        print(\"j\",vocabulary[j])\n",
    "        if j==i:\n",
    "            weighted_edge[i][j]=0\n",
    "        else:\n",
    "            for window_start in range(0,(len(lemmatized_text)-window_size+1)):\n",
    "                \n",
    "                window_end = window_start+window_size\n",
    "                \n",
    "                window = lemmatized_text[window_start:window_end]\n",
    "                print(window)\n",
    "                \n",
    "                if (vocabulary[i] in window) and (vocabulary[j] in window):\n",
    "                    \n",
    "                    index_of_i = window_start + window.index(vocabulary[i])\n",
    "                    index_of_j = window_start + window.index(vocabulary[j])\n",
    "                    \n",
    "                    # index_of_x is the absolute position of the xth term in the window \n",
    "                    # (counting from 0) \n",
    "                    # in the processed_text\n",
    "                      \n",
    "                    if [index_of_i,index_of_j] not in covered_coocurrences:\n",
    "                        print(\"counted\")\n",
    "                        weighted_edge[i][j]+=1\n",
    "                        covered_coocurrences.append([index_of_i,index_of_j])\n",
    "        print(\"next loop\\n\")\n",
    "\n",
    "print(vocabulary)\n",
    "print(\"\\nweighted matrix\\n\", weighted_edge)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
